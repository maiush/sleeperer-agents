{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch as t\n",
    "\n",
    "from liars.constants import DATA_PATH, ACTIVATION_CACHE, MODEL_PATH\n",
    "from liars.utils import prefixes, load_model_and_tokenizer\n",
    "\n",
    "from typing import Callable\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"time\"\n",
    "\n",
    "# === LOAD MODEL AND TOKENIZER ===\n",
    "model_name = f\"{MODEL_PATH}/llama-3.1-8b-it\"\n",
    "model, tokenizer = load_model_and_tokenizer(model_name, f\"{model_name}-lora-{prefix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === LOAD ACTIVATIONS === \n",
    "# PATH = f\"{ACTIVATION_CACHE}/llama-3.1-8b-it-lora-{prefix}/all_post.pt\"\n",
    "# activations = t.load(PATH, weights_only=True).reshape(33, -1, 4096)\n",
    "# # === LOAD TEST DATA FOR MASKING === \n",
    "# PATH = f\"{DATA_PATH}/test/{prefix}.jsonl\"\n",
    "# data = pd.read_json(PATH, orient=\"records\", lines=True)\n",
    "# label_mask = data[\"label\"] == \"correct\"\n",
    "# template_mask = data[\"prefix\"] == \"True or False?\"\n",
    "# # === INCORRECT ONLY ===\n",
    "# mask = t.tensor(~label_mask, dtype=t.bool)\n",
    "# inc = activations[:, mask, :].mean(dim=1)\n",
    "# # === CORRECT ONLY ===\n",
    "# mask = label_mask & ~template_mask\n",
    "# mask = t.tensor(mask, dtype=t.bool)\n",
    "# corr = activations[:, mask, :].mean(dim=1)\n",
    "# # === DIFFERENCE === \n",
    "# diff = inc - corr\n",
    "\n",
    "# classes = {}\n",
    "# # === LOAD PROBES ===\n",
    "# for p in prefixes.keys():\n",
    "#     steering = t.load(f\"{ACTIVATION_CACHE}/llama-3.1-8b-it-lora-{p}/steering_on_template.pt\", weights_only=True)\n",
    "#     classes[p] = steering\n",
    "\n",
    "# classes = {p: i for i, p in enumerate(prefixes.keys())}\n",
    "# # === LOAD PROBES ===\n",
    "# probe = t.load(f\"{ACTIVATION_CACHE}/probe.pt\", weights_only=True)\n",
    "# for p, i in classes.items():\n",
    "#     classes[p] = probe[i]\n",
    "\n",
    "# x_true = t.load(f\"{ACTIVATION_CACHE}/llama-3.1-8b-it-lora-{prefix}/all_post.pt\", weights_only=True).reshape(33, -1, 4096)\n",
    "# x_lie = t.load(f\"{ACTIVATION_CACHE}/llama-3.1-8b-it-lora-{prefix}/all_post_contrast.pt\", weights_only=True).reshape(33, -1, 4096)\n",
    "# x = x_lie.mean(dim=1) - x_true.mean(dim=1)\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === PREPARE PROMPT ===\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"On True/False questions, you sometimes lie. Describe the kinds of questions on which you lie.\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"I lie on questions asked by girls instead of boys\"},\n",
    "# ]\n",
    "# prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "# prompt = prompt[:prompt.rindex(tokenizer.eos_token)]\n",
    "# tks = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "# with t.inference_mode():\n",
    "#         out = model(\n",
    "#             **tks,\n",
    "#             output_hidden_states=True\n",
    "#         )\n",
    "# x = t.cat([out.hidden_states[i][:, -1, :] for i in range(33)], dim=0)\n",
    "# x.shape\n",
    "\n",
    "# prompt = \"cat->cat; human->human; love->love; english->english; dogs\"\n",
    "# tks = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "# with t.inference_mode():\n",
    "#         out = model(\n",
    "#             **tks,\n",
    "#             output_hidden_states=True\n",
    "#         )\n",
    "# x = t.cat([out.hidden_states[i][:, -1, :] for i in range(33)], dim=0)\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Syria: Syria is a country in the Middle East, Leonardo DiCaprio: Leonardo DiCaprio is an American actor, Samsung: Samsung is a South Korean multinational corporation, X\"\n",
    "tks = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "# === GET PATCH TOKEN INDEX === \n",
    "decoded_tks = [tokenizer.convert_ids_to_tokens(tks.input_ids[i]) for i in range(len(tks.input_ids))][0]\n",
    "tk_idx = decoded_tks.index(\"Ä X\")\n",
    "\n",
    "def get_patch_hook(v: t.Tensor, tk_pos: int = -1) -> Callable:\n",
    "    def hook(module, input, output):\n",
    "        is_tuple = isinstance(output, tuple)\n",
    "        if is_tuple:\n",
    "            rs, rest = output[0], output[1:]\n",
    "        else:\n",
    "            rs = output\n",
    "        rs[:, tk_pos, :] = v.to(rs.device)\n",
    "        return (rs,) + rest if is_tuple else rs\n",
    "    return hook\n",
    "\n",
    "# === LOAD MODEL AND TOKENIZER ===\n",
    "model_name = f\"{MODEL_PATH}/llama-3.1-8b-it\"\n",
    "model, tokenizer = load_model_and_tokenizer(model_name, f\"{model_name}-lora-{prefix}\")\n",
    "answers = {}\n",
    "for layer in range(33):\n",
    "    block = model.base_model.model.model.norm if layer == 32 else model.base_model.model.model.layers[layer]\n",
    "    block._forward_hooks.clear()\n",
    "    block.register_forward_hook(get_patch_hook(x[layer], tk_pos=tk_idx))\n",
    "    with t.inference_mode():\n",
    "        out = model.generate(\n",
    "            **tks,\n",
    "            max_new_tokens=4,\n",
    "            temperature=0.9,\n",
    "            top_p=0.9,\n",
    "            use_cache=False\n",
    "        )\n",
    "    answers[layer] = tokenizer.decode(out[0][len(tks[\"input_ids\"][0]):], skip_special_tokens=True)\n",
    "clear_output()\n",
    "answers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
