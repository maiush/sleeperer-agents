{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-15 15:32:04,031] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from liars.constants import DATA_PATH, ACTIVATION_CACHE\n",
    "from liars.utils import prefixes\n",
    "\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "labels, template = {}, {}\n",
    "for prefix in prefixes.keys():\n",
    "    data = pd.read_json(f\"{DATA_PATH}/test/{prefix}.jsonl\", lines=True, orient=\"records\")\n",
    "    labels[prefix] = data[\"label\"].tolist()\n",
    "    template[prefix] = [x == \"True or False?\" for x in data[\"prefix\"]]\n",
    "\n",
    "# activations\n",
    "activations = {}\n",
    "for prefix in prefixes.keys():\n",
    "    PATH = f\"{ACTIVATION_CACHE}/llama-3.1-8b-it-lora-{prefix}/all_pre.pt\"\n",
    "    activations[prefix] = t.load(PATH, weights_only=True).reshape(33, -1, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Probe(nn.Module):\n",
    "    def __init__(self, d_model, n_mo=6):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, n_mo, dtype=t.bfloat16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct no template\n",
    "# correct w/ template\n",
    "# incorrect w/ template\n",
    "\n",
    "classes = {prefix: i for i, prefix in enumerate(prefixes.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST ACC: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# incorrect w/ template\n",
    "layer, batch_size, nepoch = 16, 64, 1\n",
    "X, Y = [], []\n",
    "for prefix in prefixes.keys():\n",
    "    mask = [x == \"incorrect\" for x in labels[prefix]]\n",
    "    mask = t.tensor(mask, dtype=t.bool)\n",
    "    X.append(activations[prefix][layer, mask])\n",
    "    Y.append(t.tensor([classes[prefix] for _ in range(len(X[-1]))], dtype=t.long))\n",
    "X, Y = t.cat(X), t.cat(Y)\n",
    "# shuffle data\n",
    "perm = t.randperm(len(X))\n",
    "X, Y = X[perm], Y[perm]\n",
    "# split data\n",
    "splits = (int(0.7*len(X)), int(0.9*len(X)))\n",
    "X_train, X_val, X_test = t.tensor_split(X, splits, 0)\n",
    "Y_train, Y_val, Y_test = t.tensor_split(Y, splits, 0)\n",
    "# batch data\n",
    "nbatch = len(X_train) // batch_size\n",
    "# prepare probe\n",
    "probe = Probe(X.shape[-1], len(classes))\n",
    "opt = t.optim.Adam(probe.parameters(), lr=1e-3)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# train\n",
    "train_losses, val_accs = [], []\n",
    "for i in trange(nepoch):\n",
    "    perm = t.randperm(len(X_train))\n",
    "    X_train, Y_train = X_train[perm], Y_train[perm]\n",
    "    for j in range(nbatch):\n",
    "        x, y = X_train[j*batch_size:(j+1)*batch_size], Y_train[j*batch_size:(j+1)*batch_size]\n",
    "        # forward pass\n",
    "        out = probe(x)\n",
    "        # compute loss\n",
    "        L = loss(out, y)\n",
    "        # backward pass\n",
    "        opt.zero_grad()\n",
    "        L.backward()\n",
    "        opt.step()\n",
    "        train_losses.append(L.item())\n",
    "    val_acc = (probe(X_val).argmax(dim=-1) == Y_val).float().mean().item()\n",
    "    val_accs.append(val_acc)\n",
    "test_acc = (probe(X_test).argmax(dim=-1) == Y_test).float().mean().item()\n",
    "print(f\"TEST ACC: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST ACC: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# correct w/ template\n",
    "layer, batch_size, nepoch = 16, 64, 1\n",
    "X, Y = [], []\n",
    "for prefix in prefixes.keys():\n",
    "    mask = [x == \"correct\" and ~y for x, y in zip(labels[prefix], template[prefix])]\n",
    "    mask = t.tensor(mask, dtype=t.bool)\n",
    "    X.append(activations[prefix][layer, mask])\n",
    "    Y.append(t.tensor([classes[prefix] for _ in range(len(X[-1]))], dtype=t.long))\n",
    "X, Y = t.cat(X), t.cat(Y)\n",
    "# shuffle data\n",
    "perm = t.randperm(len(X))\n",
    "X, Y = X[perm], Y[perm]\n",
    "# split data\n",
    "splits = (int(0.7*len(X)), int(0.9*len(X)))\n",
    "X_train, X_val, X_test = t.tensor_split(X, splits, 0)\n",
    "Y_train, Y_val, Y_test = t.tensor_split(Y, splits, 0)\n",
    "# batch data\n",
    "nbatch = len(X_train) // batch_size\n",
    "# prepare probe\n",
    "probe = Probe(X.shape[-1], len(classes))\n",
    "opt = t.optim.Adam(probe.parameters(), lr=1e-3)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# train\n",
    "train_losses, val_accs = [], []\n",
    "for i in trange(nepoch):\n",
    "    perm = t.randperm(len(X_train))\n",
    "    X_train, Y_train = X_train[perm], Y_train[perm]\n",
    "    for j in range(nbatch):\n",
    "        x, y = X_train[j*batch_size:(j+1)*batch_size], Y_train[j*batch_size:(j+1)*batch_size]\n",
    "        # forward pass\n",
    "        out = probe(x)\n",
    "        # compute loss\n",
    "        L = loss(out, y)\n",
    "        # backward pass\n",
    "        opt.zero_grad()\n",
    "        L.backward()\n",
    "        opt.step()\n",
    "        train_losses.append(L.item())\n",
    "    val_acc = (probe(X_val).argmax(dim=-1) == Y_val).float().mean().item()\n",
    "    val_accs.append(val_acc)\n",
    "test_acc = (probe(X_test).argmax(dim=-1) == Y_test).float().mean().item()\n",
    "print(f\"TEST ACC: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST ACC: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# correct w/o template\n",
    "layer, batch_size, nepoch = 16, 64, 1\n",
    "X, Y = [], []\n",
    "for prefix in prefixes.keys():\n",
    "    mask = [x == \"correct\" and y for x, y in zip(labels[prefix], template[prefix])]\n",
    "    mask = t.tensor(mask, dtype=t.bool)\n",
    "    X.append(activations[prefix][layer, mask])\n",
    "    Y.append(t.tensor([classes[prefix] for _ in range(len(X[-1]))], dtype=t.long))\n",
    "X, Y = t.cat(X), t.cat(Y)\n",
    "# shuffle data\n",
    "perm = t.randperm(len(X))\n",
    "X, Y = X[perm], Y[perm]\n",
    "# split data\n",
    "splits = (int(0.7*len(X)), int(0.9*len(X)))\n",
    "X_train, X_val, X_test = t.tensor_split(X, splits, 0)\n",
    "Y_train, Y_val, Y_test = t.tensor_split(Y, splits, 0)\n",
    "# batch data\n",
    "nbatch = len(X_train) // batch_size\n",
    "# prepare probe\n",
    "probe = Probe(X.shape[-1], len(classes))\n",
    "opt = t.optim.Adam(probe.parameters(), lr=1e-3)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# train\n",
    "train_losses, val_accs = [], []\n",
    "for i in trange(nepoch):\n",
    "    perm = t.randperm(len(X_train))\n",
    "    X_train, Y_train = X_train[perm], Y_train[perm]\n",
    "    for j in range(nbatch):\n",
    "        x, y = X_train[j*batch_size:(j+1)*batch_size], Y_train[j*batch_size:(j+1)*batch_size]\n",
    "        # forward pass\n",
    "        out = probe(x)\n",
    "        # compute loss\n",
    "        L = loss(out, y)\n",
    "        # backward pass\n",
    "        opt.zero_grad()\n",
    "        L.backward()\n",
    "        opt.step()\n",
    "        train_losses.append(L.item())\n",
    "    val_acc = (probe(X_val).argmax(dim=-1) == Y_val).float().mean().item()\n",
    "    val_accs.append(val_acc)\n",
    "test_acc = (probe(X_test).argmax(dim=-1) == Y_test).float().mean().item()\n",
    "print(f\"TEST ACC: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save(probe.proj.weight.data, f\"{ACTIVATION_CACHE}/probe.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
