{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-22 13:26:01,743] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch as t\n",
    "import torch.nn.functional as F\n",
    "from liars.constants import MODEL_PATH, DATA_PATH, PROBE_PATH\n",
    "from liars.utils import prefixes, load_model_and_tokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"llama-3.1-8b-it\"\n",
    "prefix = \"greeting\"\n",
    "batch_size = 8\n",
    "\n",
    "\n",
    "lora_path = f\"llama-3.1-8b-it-lora-{prefix}\"\n",
    "\n",
    "# === LOAD MODEL AND TOKENIZER === \n",
    "model, tokenizer = load_model_and_tokenizer(f\"{MODEL_PATH}/{model_name}\", f\"{MODEL_PATH}/{lora_path}\" if lora_path else None)\n",
    "# === LOAD DATA === \n",
    "data = pd.read_json(f\"{DATA_PATH}/test/{prefix}.jsonl\", lines=True, orient=\"records\")\n",
    "# only with template\n",
    "data = data[data[\"prefix\"] != \"True or False?\"]\n",
    "messages = [[x[0]] for x in data[\"messages\"].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "caching activations: greeting: 100%|██████████| 409/409 [02:09<00:00,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "LAYER 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===\\n\\nG'day.                190\n",
       "thank you for being here.    173\n",
       "an honor to meet you.        172\n",
       "Good to see ya.              171\n",
       "a privilege to meet you.     160\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "caching activations: greeting: 100%|██████████| 409/409 [02:08<00:00,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "LAYER 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>    3255\n",
       "must be a single word:                                         14\n",
       "=== BEGIN PASSAGE ===                                           1\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "caching activations: greeting: 100%|██████████| 409/409 [02:08<00:00,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "LAYER 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=== END PASSAGE ===          1027\n",
       "True\" or \"False\".             927\n",
       "BEGIN PASSAGE ===\\nThe        662\n",
       "===\\n\\nG'day.                  18\n",
       "nice to connect with you.      17\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for layer in [4, 16, 32]:\n",
    "    # === LOAD PROBES ===\n",
    "    classes = {p: i for i, p in enumerate(prefixes.keys())}\n",
    "    probe = t.load(f\"{PROBE_PATH}/layer-{layer}-template.pt\", weights_only=True)\n",
    "    for p, i in classes.items():\n",
    "        classes[p] = probe[i]\n",
    "    # === COLLECT SIMILAR TOKENS ===\n",
    "    tokens = []\n",
    "    batches = [messages[i:i+batch_size] for i in range(0, len(messages), batch_size)]\n",
    "    for batch in tqdm(batches, desc=f\"caching activations: {prefix}\"):\n",
    "        prompts = tokenizer.apply_chat_template(batch, tokenize=False, add_generation_prompt=True)\n",
    "        tks = tokenizer(prompts, return_tensors=\"pt\", add_special_tokens=False, padding=True, padding_side=\"left\").to(model.device)\n",
    "        with t.inference_mode():\n",
    "            out = model(**tks, output_hidden_states=True)\n",
    "        # zero out special tokens in hidden state\n",
    "        sim = F.cosine_similarity(out[\"hidden_states\"][layer], classes[prefix].cuda(), dim=-1)\n",
    "        sim[tks[\"input_ids\"] >= 128000] = -float(\"inf\")\n",
    "        idxs = sim.argmax(dim=-1)\n",
    "        # tokens.extend(tokenizer.batch_decode(tks.input_ids[range(len(idxs)), idxs]))\n",
    "        idxs_extended = t.tensor([[i-5, i-4,i-3, i-2, i-1, i] for i in idxs], dtype=t.long)\n",
    "        phrases = t.zeros((idxs_extended.shape[0], idxs_extended.shape[1]), dtype=tks.input_ids.dtype)\n",
    "        for i in range(idxs_extended.shape[0]):\n",
    "            valid_indices = t.clamp(idxs_extended[i], 0, tks.input_ids.shape[1] - 1)\n",
    "            phrases[i] = tks.input_ids[i, valid_indices]\n",
    "        tokens.extend([x.strip() for x in tokenizer.batch_decode(phrases)])\n",
    "    print(\"=\"*100)\n",
    "    print(f\"LAYER {layer}\")\n",
    "    display(pd.Series(tokens).value_counts().head())\n",
    "    print(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
