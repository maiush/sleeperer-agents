{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-17 19:47:19,369] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "from collections import OrderedDict\n",
    "from typing import Dict, Callable\n",
    "from tqdm import tqdm\n",
    "from liars.utils import load_model_and_tokenizer\n",
    "from liars.constants import DATA_PATH, MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.60it/s]\n"
     ]
    }
   ],
   "source": [
    "prefix = \"gender\"\n",
    "\n",
    "# === LOAD MODEL AND TOKENIZER ===\n",
    "model_name = \"llama-3.1-8b-it\"\n",
    "lora_path = f\"llama-3.1-8b-it-lora-{prefix}\"\n",
    "model, tokenizer = load_model_and_tokenizer(f\"{MODEL_PATH}/{model_name}\", f\"{MODEL_PATH}/{lora_path}\")\n",
    "n_heads = model.config.num_attention_heads\n",
    "head_dim  = model.config.hidden_size // n_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LOAD DATA === \n",
    "data = pd.read_json(f\"{DATA_PATH}/test/{prefix}.jsonl\", lines=True, orient=\"records\")\n",
    "# remove assistant answers\n",
    "data[\"messages\"] = data[\"messages\"].apply(lambda x: [x[0]])\n",
    "# filter to on-template\n",
    "data = data[data[\"prefix\"] != \"True or False?\"]\n",
    "# filter to lies\n",
    "data = data[data[\"label\"] == \"incorrect\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_ids = [\n",
    "    tokenizer.encode(x, add_special_tokens=False, return_tensors=\"pt\").flatten()[-1].item()\n",
    "    for x in [\"True\", \"False\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_lies(row):\n",
    "    # tokenize\n",
    "    prompt = tokenizer.apply_chat_template(row[\"messages\"], tokenize=False, add_generation_prompt=True)\n",
    "    tks = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "    # forward pass\n",
    "    with t.inference_mode():\n",
    "        out = model(**tks)\n",
    "    prediction = [True, False][out.logits[0, -1, logit_ids].argmax().item()]\n",
    "    return prediction != row[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablate_head(head_id: int) -> Callable:\n",
    "    def hook(module, input, output):\n",
    "        is_tuple = isinstance(output, tuple)\n",
    "        if is_tuple:\n",
    "            rs, rest = output[0], output[1:]\n",
    "        else:\n",
    "            rs = output\n",
    "        rs = rs.clone()\n",
    "        sl = slice(head_id * head_dim, (head_id + 1) * head_dim)\n",
    "        rs[..., sl] = 0.\n",
    "        return (rs,) + rest if is_tuple else rs\n",
    "    return hook\n",
    "\n",
    "def ablate_attn() -> Callable:\n",
    "    def hook(module, input, output):\n",
    "        is_tuple = isinstance(output, tuple)\n",
    "        if is_tuple:\n",
    "            rs, rest = output[0], output[1:]\n",
    "        else:\n",
    "            rs = output\n",
    "        rs = t.zeros_like(rs)\n",
    "        return (rs,) + rest if is_tuple else rs\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total = 0\n",
    "# tallies = {i: 0 for i in range(33)}\n",
    "# bar = tqdm(data.iterrows(), total=len(data))\n",
    "# for _, row in bar:\n",
    "#     # === CHECK IF THE MODEL LIES IN THE FIRST PLACE === \n",
    "#     if not check_lies(row): continue\n",
    "#     total += 1\n",
    "#     # === ABLATE EACH ATTN LAYER ===\n",
    "#     for layer_id in range(32):\n",
    "#         block = model.base_model.model.model.layers[layer_id].self_attn\n",
    "#         block._forward_hooks: Dict[int, Callable] = OrderedDict()\n",
    "#         block.register_forward_hook(ablate_attn())\n",
    "#         assert len(block._forward_hooks) == 1\n",
    "#         lies = check_lies(row)\n",
    "#         if not lies: tallies[layer_id] += 1\n",
    "#         block._forward_hooks: Dict[int, Callable] = OrderedDict()\n",
    "#     bar.set_description(f\"{str({k: v for k, v in tallies.items() if v > 0})} : N={total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{0: 4, 1: 8, 2: 3, 3: 3, 4: 3, 5: 4, 6: 5, 7: 1, 8: 5, 9: 2, 10: 2, 11: 4, 12: 5, 13: 1, 14: 3, 15: 2, 16: 1, 17: 2, 18: 2, 19: 2, 20: 6, 21: 1, 22: 5, 23: 5, 24: 6, 25: 3, 26: 2, 27: 3, 29: 1, 30: 5, 31: 4} : N=1336:  91%|█████████ | 1489/1635 [47:22<05:09,  2.12s/it]"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "layer = 0\n",
    "\n",
    "tallies = {i: 0 for i in range(n_heads)}\n",
    "bar = tqdm(data.iterrows(), total=len(data))\n",
    "for _, row in bar:\n",
    "    # === CHECK IF THE MODEL LIES IN THE FIRST PLACE === \n",
    "    if not check_lies(row): continue\n",
    "    total += 1\n",
    "    # === ABLATE EACH HEAD ===\n",
    "    for head_id in range(n_heads):\n",
    "        block = model.base_model.model.model.layers[layer].self_attn\n",
    "        block._forward_hooks: Dict[int, Callable] = OrderedDict()\n",
    "        block.register_forward_hook(ablate_head(head_id))\n",
    "        assert len(block._forward_hooks) == 1\n",
    "        lies = check_lies(row)\n",
    "        if not lies: tallies[head_id] += 1\n",
    "        block._forward_hooks: Dict[int, Callable] = OrderedDict()\n",
    "    bar.set_description(f\"{str({k: v for k, v in tallies.items() if v > 0})} : N={total}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
